import os
from typing import List, Dict, Any, Optional, Tuple, Type
from pydantic import BaseModel, Field
import numpy as np
from opik import track

from app.Agents_services.base_agents import BaseAgent
from app.vector_db_service.vector_database import DocumentChunk, SearchResult
from app.vector_db_service.clients.chromadb import ChromaDBClient
from app.middleware.database import FAQEntry, SessionLocal
from sqlalchemy.orm import Session
from sqlalchemy import func
from app.Agents.query_rewrite_agent import QueryRewriteAgent
from app.Agents.query_expansion import QueryExpansionAgent

class RAGResponse(BaseModel):
    answer: str = Field(..., description="The answer generated by the RAG agent")
    cited_chunks: List[Dict[str, Any]] = Field(default_factory=list, description="List of chunks cited or used for current query")

class ConversationalRAGAgent:
    """
    Conversational RAG Agent that uses vector search and LLM to answer questions
    based on user's FAQ documents.
    """
    
    def __init__(
        self, 
        llm_agent: BaseAgent,
        vector_db_client: Optional[ChromaDBClient] = None,
        collection_prefix: str = "user_faq_",
        embedding_dimension: int = 384,  # Default for all-MiniLM-L6-v2
        max_chunks: int = 5
    ):
        self.llm_agent = llm_agent
        self.vector_db = vector_db_client or ChromaDBClient(persistence_path="./chroma_db")
        self.vector_db.connect()
        self.collection_prefix = collection_prefix
        self.embedding_dimension = embedding_dimension
        self.max_chunks = max_chunks
        self.db = SessionLocal()
        self.query_rewrite_agent = QueryRewriteAgent(llm_agent)
        self.query_expansion_agent = QueryExpansionAgent(llm_agent)
        
        self.system_prompt_template = """
You are Nexus Assistant, a helpful and empathetic AI assistant for AI-Nexus.

Your primary role is to answer questions based on the FAQ documents provided by the user.
You should always be polite, respectful, and provide accurate information based on the knowledge provided.

Here are the relevant FAQ sections that might help answer the user's question:

{knowledge_chunks}

Guidelines for your responses:
1. Always cite the specific chunks you used to answer by referencing their numbers like (1), (2), etc.
2. If the user's question isn't covered by the provided chunks, politely explain that you don't have that specific information.
3. For greetings or gratitude, respond naturally without needing to cite chunks.
4. Never make up information that isn't in the provided chunks.
5. If you're unsure about something, it's okay to say so rather than guessing.
6. Format your responses in a clear, readable way.
7. Be empathetic and understanding of the user's needs.

Remember, you are Nexus Assistant, and your purpose is to help users find information from their FAQ documents.
"""

    @track
    async def ensure_collection_exists(self, user_id: str) -> bool:
        """
        Check if collection exists for user, if not create and populate it from FAQs
        """
        collection_name = f"{self.collection_prefix}{user_id}"
        # Create collection
        success = self.vector_db.create_collection(collection_name, self.embedding_dimension)
        if not success:
            return False
            
        # Get FAQs for user
        from app.middleware.database import FinetuneJob
        fine_tune_jobs = self.db.query(FinetuneJob).filter(FinetuneJob.user_id == user_id).all()
        job_ids = []
        for job in fine_tune_jobs:
            job_ids.extend(job.job_ids.split(","))
        faqs = self.db.query(FAQEntry).filter(FAQEntry.job_id.in_(job_ids)).all()
        if not faqs:
            return False
            
        # Convert FAQs to document chunks
        chunks = []
        for i, faq in enumerate(faqs):
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer('all-MiniLM-L6-v2')
            
            text = f"Q: {faq.question}\nA: {faq.answer}"
            embedding = model.encode(text).tolist()
            
            # Create document chunk
            chunk = DocumentChunk(
                id=f"{faq.id}",
                text=text,
                embedding=embedding,
                metadata={
                    "section": faq.section,
                    "question": faq.question,
                    "chunk_index": i
                }
            )
            chunks.append(chunk)
            
        # Ingest chunks
        success = self.vector_db.ingest_documents(collection_name, chunks)
        return success
    
    @track
    async def enhance_query(self, query: str, chat_history: List[Tuple[str, str]] = None) -> List[str]:
        """
        Enhance the query using both rewrite and expansion agents
        """
        rewritten_query = await self.query_rewrite_agent.rewrite_query(query, chat_history)
        expanded_queries = await self.query_expansion_agent.expand_query(rewritten_query)
        
        all_queries = [query]
        if rewritten_query != query:
            all_queries.append(rewritten_query)
        
        for eq in expanded_queries:
            if eq not in all_queries:
                all_queries.append(eq)
                
        return all_queries
    
    @track
    async def search_relevant_chunks(self, user_id: str, query: str, chat_history: List[Tuple[str, str]] = None) -> List[SearchResult]:
        """
        Search for relevant chunks in the vector database using enhanced queries
        """
        collection_name = f"{self.collection_prefix}{user_id}"
    
        exists = await self.ensure_collection_exists(user_id)
        if not exists:
            return []
        
        enhanced_queries = await self.enhance_query(query, chat_history)
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2')
        
        all_results = []
        for enhanced_query in enhanced_queries:
            # Generate embedding for query
            query_embedding = model.encode(enhanced_query).tolist()
            
            # Search for similar chunks
            results = self.vector_db.search(
                collection_name=collection_name,
                query_vector=query_embedding,
                top_k=self.max_chunks
            )
            all_results.extend(results)
    
        unique_results = {}
        for result in all_results:
            if result.id not in unique_results or result.score > unique_results[result.id].score:
                unique_results[result.id] = result

        # Sort by score and take top k or Reranking
        sorted_results = sorted(unique_results.values(), key=lambda x: x.score, reverse=True)
        return sorted_results[:self.max_chunks]
    
    @track
    def format_knowledge_chunks(self, chunks: List[SearchResult]) -> str:
        """
        Format knowledge chunks for inclusion in the system prompt
        """
        if not chunks:
            return "No relevant information found in the FAQ documents."
            
        formatted_chunks = []
        for i, chunk in enumerate(chunks):
            section = chunk.metadata.get("section", "General")
            formatted_chunk = f"Chunk ({i+1}) [Section: {section}]:\n{chunk.text}\n"
            formatted_chunks.append(formatted_chunk)
            
        return "\n".join(formatted_chunks)
    
    @track
    async def chat(self, user_id: str, message: str, chat_history: List[Tuple[str, str]] = None) -> Dict[str, Any]:
        """
        Main method to chat with the RAG agent
        """
        if chat_history is None:
            chat_history = []
            
        # Search for relevant chunks using enhanced queries
        chunks = await self.search_relevant_chunks(user_id, message, chat_history)
        
        # Format chunks for system prompt
        formatted_chunks = self.format_knowledge_chunks(chunks)
        
        # Create system prompt with knowledge chunks
        system_prompt = self.system_prompt_template.format(knowledge_chunks=formatted_chunks)
        
        # Get response from LLM
        response = await self.llm_agent.run_async(
            system_prompt=system_prompt,
            user_input=message,
            chat_history=chat_history,
            response_model=RAGResponse
        )
        
        # Extract cited chunks
        cited_chunks = []
        for i, chunk in enumerate(chunks):
            # Check if the chunk number is mentioned in the response
            if f"({i+1})" in response["cited_chunks"]:
                cited_chunks.append({
                    "id": chunk.id,
                    "text": chunk.text,
                    "section": chunk.metadata.get("section", "General"),
                    "score": chunk.score
                })
        
        return {
            "content": response["answer"],
            "cited_chunks": cited_chunks
        }
